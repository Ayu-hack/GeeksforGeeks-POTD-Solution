{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Music Generator based on Genre using VanillaGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting librosa\n",
      "  Downloading librosa-0.10.2.post1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in c:\\users\\cl501_29\\appdata\\roaming\\python\\python311\\site-packages (from librosa) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa) (1.2.2)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa) (1.2.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa) (0.59.0)\n",
      "Collecting soundfile>=0.12.1 (from librosa)\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-win_amd64.whl.metadata (16 kB)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Downloading soxr-0.5.0.post1-cp311-cp311-win_amd64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa) (4.9.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in c:\\programdata\\anaconda3\\lib\\site-packages (from numba>=0.51.0->librosa) (0.42.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pooch>=1.1->librosa) (3.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pooch>=1.1->librosa) (23.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pooch>=1.1->librosa) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->librosa) (2.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.2.2)\n",
      "Downloading librosa-0.10.2.post1-py3-none-any.whl (260 kB)\n",
      "   ---------------------------------------- 0.0/260.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 260.1/260.1 kB 5.4 MB/s eta 0:00:00\n",
      "Downloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "   ---------------------------------------- 0.0/64.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 64.6/64.6 kB ? eta 0:00:00\n",
      "Downloading soundfile-0.13.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 0.4/1.0 MB 13.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.0/1.0 MB 12.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 10.7 MB/s eta 0:00:00\n",
      "Downloading soxr-0.5.0.post1-cp311-cp311-win_amd64.whl (166 kB)\n",
      "   ---------------------------------------- 0.0/166.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 166.7/166.7 kB 10.4 MB/s eta 0:00:00\n",
      "Installing collected packages: soxr, audioread, soundfile, pooch, librosa\n",
      "Successfully installed audioread-3.0.1 librosa-0.10.2.post1 pooch-1.8.2 soundfile-0.13.1 soxr-0.5.0.post1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to dataset\n",
    "DATA_PATH = r'E:\\Deep Learning\\Music Generator by Genre\\GTZAN Data\\genres_original'\n",
    "\n",
    "# Define fixed length for spectrograms (e.g., 128 time frames)\n",
    "FIXED_LENGTH = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and process Mel-Spectrograms for a specific genre\n",
    "def load_data(genre, sr=22050, n_fft=2048, hop_length=512, n_mels=128):\n",
    "    genre_path = os.path.join(DATA_PATH, genre)\n",
    "    mel_spectrograms = []\n",
    "\n",
    "    for file_name in os.listdir(genre_path):\n",
    "        file_path = os.path.join(genre_path, file_name)\n",
    "        if file_name.endswith('.wav'):\n",
    "            y, _ = librosa.load(file_path, sr=sr)\n",
    "            mel = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "            mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "\n",
    "            # Fix the shape of spectrograms (padding or truncating)\n",
    "            if mel_db.shape[1] < FIXED_LENGTH:\n",
    "                # Pad with zeros if shorter than FIXED_LENGTH\n",
    "                pad_width = FIXED_LENGTH - mel_db.shape[1]\n",
    "                mel_db = np.pad(mel_db, ((0, 0), (0, pad_width)), mode='constant')\n",
    "            else:\n",
    "                # Truncate if longer than FIXED_LENGTH\n",
    "                mel_db = mel_db[:, :FIXED_LENGTH]\n",
    "\n",
    "            mel_spectrograms.append(mel_db)\n",
    "\n",
    "    return np.array(mel_spectrograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(mel_spectrograms):\n",
    "    mel_spectrograms = (mel_spectrograms - mel_spectrograms.min()) / (mel_spectrograms.max() - mel_spectrograms.min())\n",
    "    mel_spectrograms = mel_spectrograms.astype('float32')\n",
    "    mel_spectrograms = mel_spectrograms[..., np.newaxis]\n",
    "    return mel_spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.56033975]\n",
      "  [0.585518  ]\n",
      "  [0.58630806]\n",
      "  ...\n",
      "  [0.5648533 ]\n",
      "  [0.52628505]\n",
      "  [0.5426846 ]]\n",
      "\n",
      " [[0.6440804 ]\n",
      "  [0.6800323 ]\n",
      "  [0.70510846]\n",
      "  ...\n",
      "  [0.65127456]\n",
      "  [0.6460861 ]\n",
      "  [0.63288176]]\n",
      "\n",
      " [[0.6583756 ]\n",
      "  [0.716168  ]\n",
      "  [0.7374484 ]\n",
      "  ...\n",
      "  [0.7215705 ]\n",
      "  [0.75937396]\n",
      "  [0.77298266]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  ...\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  ...\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  ...\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]]\n",
      "Classical Genre Mel Spectrograms shape: (100, 128, 1000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data for a specific genre\n",
    "genre = \"classical\" \n",
    "mel_spectrograms = load_data(genre)\n",
    "mel_spectrograms = preprocess_data(mel_spectrograms)\n",
    "\n",
    "print(f'{mel_spectrograms[0]}')\n",
    "\n",
    "print(f'Classical Genre Mel Spectrograms shape: {mel_spectrograms.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 1000, 1)\n"
     ]
    }
   ],
   "source": [
    "# GAN Parameters\n",
    "latent_dim = 100\n",
    "input_shape = mel_spectrograms.shape[1:]\n",
    "\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Generator Model\n",
    "\n",
    "def build_generator(latent_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "\n",
    "        # project noise vector to initial feature map\n",
    "        layers.Dense(512 * 4 *4, input_dim=latent_dim),\n",
    "        layers.Reshape((4,4,512)),\n",
    "\n",
    "        # Upsampling layers with increasing resolution\n",
    "        layers.Conv2DTranspose(256, kernel_size=5, strides=(2, 2), padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "\n",
    "        layers.Conv2DTranspose(128, kernel_size=5, strides=(2, 5), padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "\n",
    "        layers.Conv2DTranspose(64, kernel_size=5, strides=(2, 5), padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "\n",
    "        layers.Conv2DTranspose(32, kernel_size=3, strides=(2, 5), padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "\n",
    "\n",
    "        # Output layer (mel-spectrogram grayscale image)\n",
    "        layers.Conv2DTranspose(1, kernel_size=5, strides=(2,1), padding='same', activation='tanh')\n",
    "\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Discriminator Model\n",
    "\n",
    "def build_discriminator(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        # First convolutional layer\n",
    "        layers.Conv2D(64, kernel_size=5, strides=2, padding='same', input_shape=input_shape),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        # Second convolutional layer\n",
    "        layers.Conv2D(128, kernel_size=5, strides=2, padding='same'),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        # Third convolutional layer\n",
    "        layers.Conv2D(256, kernel_size=5, strides=2, padding='same'),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        # Fourth convolutional layer\n",
    "        layers.Conv2D(512, kernel_size=5, strides=2, padding='same'),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        layers.Flatten(),\n",
    "\n",
    "        # Dense layers for classification\n",
    "        layers.Dense(1024),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dropout(0.5),\n",
    "\n",
    "        layers.Dense(1, activation='sigmoid')  # Binary classification (real/fake)\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Generator and Discriminator\n",
    "generator = build_generator(latent_dim)\n",
    "discriminator = build_discriminator(input_shape)\n",
    "\n",
    "# Compile Discriminator\n",
    "discriminator.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), \n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build GAN Model\n",
    "discriminator.trainable = False\n",
    "gan_input = layers.Input(shape=(latent_dim,))\n",
    "fake_image = generator(gan_input)\n",
    "gan_output = discriminator(fake_image)\n",
    "gan = tf.keras.Model(gan_input, gan_output)\n",
    "gan.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), \n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100)]             0         \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, 128, 1000, 1)      5149825   \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (None, 1)                 268546561 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 273,696,386\n",
      "Trainable params: 5,148,865\n",
      "Non-trainable params: 268,547,521\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train_gan(generator, discriminator, gan, data, epochs, batch_size):\n",
    "    half_batch = batch_size // 2\n",
    "    for epoch in range(epochs):\n",
    "        # Train Discriminator\n",
    "        idx = np.random.randint(0, data.shape[0], half_batch)\n",
    "        real_images = tf.convert_to_tensor(data[idx], dtype=tf.float32)\n",
    "        real_labels = np.ones((half_batch, 1))\n",
    "\n",
    "        noise = np.random.normal(0, 1, (half_batch, latent_dim))\n",
    "        fake_images = generator.predict(noise)\n",
    "        fake_labels = np.zeros((half_batch, 1))\n",
    "\n",
    "        d_loss_real = discriminator.train_on_batch(real_images, real_labels)\n",
    "        d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n",
    "\n",
    "        # Train Generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        misleading_labels = np.ones((batch_size, 1))\n",
    "        g_loss = gan.train_on_batch(noise, misleading_labels)\n",
    "\n",
    "        # Log losses\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs} | D Loss: {d_loss_real[0]+d_loss_fake[0]:.4f} | G Loss: {g_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 923ms/step\n",
      "Epoch 0/50 | D Loss: 1.3933 | G Loss: 0.0004\n",
      "1/1 [==============================] - 1s 650ms/step\n",
      "1/1 [==============================] - 1s 629ms/step\n",
      "1/1 [==============================] - 1s 645ms/step\n",
      "1/1 [==============================] - 1s 601ms/step\n",
      "1/1 [==============================] - 1s 679ms/step\n",
      "1/1 [==============================] - 1s 635ms/step\n",
      "1/1 [==============================] - 1s 601ms/step\n",
      "1/1 [==============================] - 1s 634ms/step\n",
      "1/1 [==============================] - 1s 668ms/step\n",
      "1/1 [==============================] - 1s 652ms/step\n",
      "Epoch 10/50 | D Loss: 0.0000 | G Loss: 35.8930\n",
      "1/1 [==============================] - 1s 612ms/step\n",
      "1/1 [==============================] - 1s 608ms/step\n",
      "1/1 [==============================] - 1s 844ms/step\n",
      "1/1 [==============================] - 1s 712ms/step\n",
      "1/1 [==============================] - 1s 564ms/step\n",
      "1/1 [==============================] - 1s 686ms/step\n",
      "1/1 [==============================] - 1s 586ms/step\n",
      "1/1 [==============================] - 1s 576ms/step\n",
      "1/1 [==============================] - 1s 602ms/step\n",
      "1/1 [==============================] - 1s 597ms/step\n",
      "Epoch 20/50 | D Loss: 0.0000 | G Loss: 72.7631\n",
      "1/1 [==============================] - 1s 601ms/step\n",
      "1/1 [==============================] - 1s 600ms/step\n",
      "1/1 [==============================] - 1s 594ms/step\n",
      "1/1 [==============================] - 1s 664ms/step\n",
      "1/1 [==============================] - 1s 579ms/step\n",
      "1/1 [==============================] - 1s 566ms/step\n",
      "1/1 [==============================] - 1s 598ms/step\n",
      "1/1 [==============================] - 1s 593ms/step\n",
      "1/1 [==============================] - 1s 607ms/step\n",
      "1/1 [==============================] - 1s 578ms/step\n",
      "Epoch 30/50 | D Loss: 0.0000 | G Loss: 1.8206\n",
      "1/1 [==============================] - 1s 601ms/step\n",
      "1/1 [==============================] - 1s 577ms/step\n",
      "1/1 [==============================] - 1s 616ms/step\n",
      "1/1 [==============================] - 1s 641ms/step\n",
      "1/1 [==============================] - 1s 617ms/step\n",
      "1/1 [==============================] - 1s 583ms/step\n",
      "1/1 [==============================] - 1s 613ms/step\n",
      "1/1 [==============================] - 1s 583ms/step\n",
      "1/1 [==============================] - 1s 575ms/step\n",
      "1/1 [==============================] - 1s 631ms/step\n",
      "Epoch 40/50 | D Loss: 0.0000 | G Loss: 0.0000\n",
      "1/1 [==============================] - 1s 584ms/step\n",
      "1/1 [==============================] - 1s 568ms/step\n",
      "1/1 [==============================] - 1s 587ms/step\n",
      "1/1 [==============================] - 1s 651ms/step\n",
      "1/1 [==============================] - 1s 618ms/step\n",
      "1/1 [==============================] - 1s 585ms/step\n",
      "1/1 [==============================] - 1s 583ms/step\n",
      "1/1 [==============================] - 1s 601ms/step\n",
      "1/1 [==============================] - 1s 590ms/step\n"
     ]
    }
   ],
   "source": [
    "# Train the GAN\n",
    "train_gan(generator, discriminator, gan, mel_spectrograms, epochs=50, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "sr = 22050\n",
    "\n",
    "# Generate New Audio\n",
    "def generate_audio(generator, latent_dim, sr=22050):\n",
    "    noise = np.random.normal(0, 1, (1, latent_dim))\n",
    "    generated_mel = generator.predict(noise)\n",
    "    generated_mel = generated_mel.squeeze()  # Remove extra dimensions\n",
    "\n",
    "    # Ensure the Mel spectrogram is in the correct floating-point format\n",
    "    generated_mel = np.clip(generated_mel, 0, 1)  # Ensure values are within range\n",
    "    generated_mel = generated_mel.astype(np.float32)  # Convert to float32 for librosa\n",
    "\n",
    "    # Convert Mel-Spectrogram back to audio\n",
    "    mel_spectrogram = librosa.feature.inverse.mel_to_audio(generated_mel, sr=sr)\n",
    "    return mel_spectrogram\n",
    "\n",
    "# Example: Generate audio\n",
    "generated_audio = generate_audio(generator, latent_dim)\n",
    "\n",
    "# Save the generated audio as a .wav file using soundfile\n",
    "sf.write('generated_audio.wav', generated_audio, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Models saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the generator model\n",
    "generator.save('generator_model.h5')\n",
    "\n",
    "# Save the discriminator model\n",
    "discriminator.save('discriminator_model.h5')\n",
    "\n",
    "# Save the GAN model (optional)\n",
    "gan.save('gan_model.h5')\n",
    "\n",
    "print(\"Models saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
